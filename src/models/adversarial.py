import os
import torch
import yaml
import librosa
import soundfile as sf
import numpy as np
from tqdm import tqdm
from easydict import EasyDict
from torch import nn

from .base import BaseAugmentor
from .aasist_ssl import Model as AasistSSL
from .aasist import Model as Aasist
from .conformer_tcm import Model as Conformer
from .ecapa_tdnn import ECAPA_TDNN
from .raw_net2 import RawNet
from .utils import librosa_to_pydub
from src.aa import utils
from src.aa.aa_types import AttackEnum
from ResNetModels.ResNetSE34V2 import MainModel
import sys
sys.path.append("/home/eoil/AGENT/next_tdnn")

SUPPORTED_CM = ["aasistssl","aasist","conformer_tcm","rawnet2"]
SUPPORTED_ASV = ["resnetse34v2","ecapa_tdnn","next_tdnn"]
SUPPORTED_ADV = [
    "CM_FGSM_0001", "CM_FGSM_0003", "CM_FGSM_0005", "CM_FGSM_0007", 
    "CM_BIM_0001", "CM_BIM_0003", "CM_BIM_0005", "CM_BIM_0007", "CM_BIM_001", "CM_BIM_004", "CM_BIM_008", "CM_BIM_012", "CM_BIM_016",
    "CM_PGD_0001", "CM_PGD_0003", "CM_PGD_0005", "CM_PGD_0007",
    "ASV_FGSM_0001", "ASV_FGSM_0003", "ASV_FGSM_0005", "ASV_FGSM_0007", 
    "ASV_BIM_0001", "ASV_BIM_0003", "ASV_BIM_0005", "ASV_BIM_0007", "ASV_BIM_001", "ASV_BIM_004", "ASV_BIM_008", "ASV_BIM_012", "ASV_BIM_016",
    "ASV_PGD_0001", "ASV_PGD_0003", "ASV_PGD_0005", "ASV_PGD_0007",
    "BOTH_DD_004", "BOTH_DD_008",
    "BOTH_AGENT_001", "BOTH_AGENT_004", "BOTH_AGENT_008", "BOTH_AGENT_012", "BOTH_AGENT_016",
    "BOTH_ReLU_004", "BOTH_ReLU_008",
    "BOTH_ReLU_00"
]

import logging

logger = logging.getLogger(__name__)

class AdversarialNoiseAugmentor(BaseAugmentor):
    """
    Adversarial noise augmentor.

    This augmentor adds adversarial noise to the input audio.
    The adversarial noise is generated by several attacks supported by ART evasion attacks:
    https://adversarial-robustness-toolbox.readthedocs.io/en/latest/index.html

    configs:
    :model_name: name of the classifier (CM) model. Supported models: ${SUPPORTED_CM}
    :model_pretrained: path to the pretrained CM model
    :config_path: path to the configuration file of the CM model
    :device: device to run the CM model (cpu or cuda)
    :adv_method: name of the adversarial methods - supported by ART evasion attacks. Currently supported methods: ${SUPPORTED_ADV}
    :adv_config: configuration of the adversarial method
    """.format(SUPPORTED_CM=SUPPORTED_CM, SUPPORTED_ADV=SUPPORTED_ADV)

    def __init__(self, config: dict, y_true: np.array = None):
        super().__init__(config, input_paths=None, file_names=None)

        self.model_name = config["model_name"]
        self.model_pretrained = config["model_pretrained"]
        self.device = config["device"]
        self.adv_method1 = config["adv_method1"]
        self.batch_size = config["batch_size"]
        self.y_true = np.array([1])
        self.input_path = config["input_path"]

        # CM or ASV
        assert self.adv_method1 in SUPPORTED_ADV, f"adv_method must be one of {SUPPORTED_ADV}"
        attack_method1, attack_params1 = AttackEnum[self.adv_method1].value
        target_module = AttackEnum[self.adv_method1].target_module

        if self.model_name == "aasist":
            with open(config["config_path"], "r") as f:
                config_path = yaml.safe_load(f)
                self.artmodel = Aasist(config_path['model_config'], device=self.device).to(self.device)
                state_dict = torch.load(self.model_pretrained, map_location=self.device)
                self.artmodel.load_state_dict(state_dict, strict=True)
                self.artmodel = nn.DataParallel(self.artmodel).to(self.device)
                self.artmodel.eval()


        elif self.model_name == "aasistssl":
            self.artmodel = AasistSSL(device=self.device)
            self.artmodel = nn.DataParallel(self.artmodel).to(self.device)

            # checkpoint key 정리
            ckpt = torch.load(self.model_pretrained, map_location=self.device)
            ckpt = {f"module.{k}" if not k.startswith("module.") else k: v for k, v in ckpt.items()}

            # 유연하게 로드
            self.artmodel.load_state_dict(ckpt, strict=False)
            self.artmodel.eval()

        elif self.model_name == "conformer_tcm":
            args = EasyDict({"emb_size": 144, "num_encoders": 4, "heads": 4, "kernel_size": 31})
            self.artmodel = Conformer(args=args, device=self.device).to(self.device)
            self.artmodel.load_state_dict(torch.load(self.model_pretrained, map_location=self.device))
            self.artmodel.eval()

        elif self.model_name == "rawnet2":
            if "model_config" not in config:
                config["model_config"] = {
                    "nb_samp": 64600, "first_conv": 1024, "in_channels": 1,
                    "filts": [20, [20, 20], [20, 128], [128, 128]],
                    "blocks": [2, 4], "nb_fc_node": 1024, "gru_node": 1024,
                    "nb_gru_layer": 3, "nb_classes": 2
                }
            self.artmodel = RawNet(d_args=config["model_config"], device=self.device).to(self.device)
            checkpoint = torch.load(self.model_pretrained, map_location=self.device)
            if "state_dict" in checkpoint:
                self.artmodel.load_state_dict(checkpoint["state_dict"])
            else:
                self.artmodel.load_state_dict(checkpoint)
            self.artmodel.train()
            for param in self.artmodel.parameters():
                param.requires_grad = False

        elif self.model_name == "resnetse34v2":
            self.artmodel = MainModel().to(self.device)
            self._load_parameters(self.artmodel, self.model_pretrained)
            self.artmodel.eval()

        elif self.model_name == "ecapa_tdnn":
            self.artmodel = ECAPA_TDNN(C=1024).to(self.device)
            checkpoint = torch.load(self.model_pretrained, map_location=self.device)

            if "model" in checkpoint:
                state_dict = checkpoint["model"]
            else:
                state_dict = checkpoint

            new_state_dict = {}
            for k, v in state_dict.items():
                if k.startswith("speaker_encoder."):
                    new_k = k.replace("speaker_encoder.", "")
                    new_state_dict[new_k] = v

            self.artmodel.load_state_dict(new_state_dict, strict=True)
            self.artmodel.eval()
        
        elif self.model_name == "next_tdnn":
            import importlib.util
            spec = importlib.util.spec_from_file_location("asv_config", "/home/eoil/AGENT/next_tdnn/NeXt_TDNN_C256_B3_K65_7_cyclical_lr_step.py")
            cfg = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(cfg)

            fe_module = importlib.import_module('preprocessing.' + cfg.FEATURE_EXTRACTOR)
            feature_extractor = fe_module.feature_extractor(**cfg.FEATURE_EXTRACTOR_CONFIG)

            model_module = importlib.import_module('models.' + cfg.MODEL)
            model = model_module.MainModel(**cfg.MODEL_CONFIG)

            agg_module = importlib.import_module('aggregation.' + cfg.AGGREGATION)
            aggregation = agg_module.Aggregation(**cfg.AGGREGATION_CONFIG)

            from SpeakerNet import SpeakerNet
            model = SpeakerNet(
                feature_extractor=feature_extractor,
                spec_aug=None,
                model=model,
                aggregation=aggregation,
                loss_function=None
            ).to(self.device)

            checkpoint = torch.load(cfg.TEST_CHECKPOINT, map_location=self.device)

            if "state_dict" in checkpoint:
                state_dict = {
                    k.replace("speaker_net.", ""): v
                    for k, v in checkpoint["state_dict"].items()
                }
            else:
                state_dict = {
                    k.replace("speaker_net.", ""): v
                    for k, v in checkpoint.items()
                }

            model.load_state_dict(state_dict, strict=True)
            model.eval()
            self.artmodel = model

        else:
            raise ValueError(f"Unsupported ASV model: {config['asv_model_name']}")


        if target_module == "ASV":
            self.asv_model = self.artmodel  
            self.adv_class1 = attack_method1(
                asv_model=self.asv_model,
                enroll_path=config["enroll_path"],
                input_path=config["input_path"],
                device=self.device,
                **attack_params1 
            )

        elif target_module == "BOTH":
            # Load ASV model separately
            if config["asv_model_name"] == "resnetse34v2":
                self.asv_model = MainModel().to(self.device)
                self._load_parameters(self.asv_model, config["asv_model_pretrained"])
                self.asv_model.eval()

            elif config["asv_model_name"] == "ecapa_tdnn":
                self.asv_model = ECAPA_TDNN(C=1024).to(self.device)
                asv_ckpt = torch.load(config["asv_model_pretrained"], map_location=self.device)
                state_dict = asv_ckpt["model"] if "model" in asv_ckpt else asv_ckpt
                new_state_dict = {
                    k.replace("speaker_encoder.", ""): v
                    for k, v in state_dict.items()
                    if k.startswith("speaker_encoder.")
                }
                self.asv_model.load_state_dict(new_state_dict, strict=True)
                self.asv_model.eval()
                
            elif config["asv_model_name"] == "next_tdnn":
                import importlib.util
                spec = importlib.util.spec_from_file_location("asv_config", "/home/eoil/AGENT/next_tdnn/NeXt_TDNN_C256_B3_K65_7_cyclical_lr_step.py")
                cfg = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(cfg)

                fe_module = importlib.import_module('preprocessing.' + cfg.FEATURE_EXTRACTOR)
                feature_extractor = fe_module.feature_extractor(**cfg.FEATURE_EXTRACTOR_CONFIG)

                model_module = importlib.import_module('models.' + cfg.MODEL)
                model = model_module.MainModel(**cfg.MODEL_CONFIG)

                agg_module = importlib.import_module('aggregation.' + cfg.AGGREGATION)
                aggregation = agg_module.Aggregation(**cfg.AGGREGATION_CONFIG)

                from SpeakerNet import SpeakerNet
                model = SpeakerNet(
                    feature_extractor=feature_extractor,
                    spec_aug=None,
                    model=model,
                    aggregation=aggregation,
                    loss_function=None
                ).to(self.device)

                checkpoint = torch.load(cfg.TEST_CHECKPOINT, map_location=self.device)

                if "state_dict" in checkpoint:
                    state_dict = {
                        k.replace("speaker_net.", ""): v
                        for k, v in checkpoint["state_dict"].items()
                    }
                else:
                    state_dict = {
                        k.replace("speaker_net.", ""): v
                        for k, v in checkpoint.items()
                    }

                model.load_state_dict(state_dict, strict=True)
                model.eval()
                self.asv_model = model 

            else:
                raise ValueError(f"Unsupported ASV model: {config['asv_model_name']}")

            self.cm_model = self.artmodel
            self.adv_class1 = attack_method1(
                asv_model=self.asv_model,
                cm_model=self.cm_model,
                enroll_path=config["enroll_path"],
                input_path=config["input_path"],
                device=self.device,
                **attack_params1
            )

        else:
            self.adv_class1 = attack_method1(
                self.artmodel,
                device=self.device,
                **attack_params1
            )

    def _load_parameters(self, model, ckpt_path):
        loaded_state = torch.load(ckpt_path, map_location='cpu')
        new_state = {}
        for k, v in loaded_state.items():
            k_clean = k.replace("module.", "").replace("speaker_encoder.", "").replace("__S__.", "")
            if k_clean in model.state_dict() and model.state_dict()[k_clean].shape == v.shape:
                new_state[k_clean] = v
        model.load_state_dict(new_state, strict=True)
        print(f"✅ Loaded {len(new_state)} weights from {ckpt_path}")



    def load_batch(self, filenames, labels):
        """
        Load audio data and corresponding labels into batches.
        """
        self.data_batch = []
        self.labels = []  # Store labels
        self.batched_filenames = []

        for filename, label in zip(filenames, labels):
            audio_data, _ = librosa.load(filename, sr=None)  # Load audio data
            self.data_batch.append(torch.tensor(audio_data).unsqueeze(0))  # Add batch dimension
            self.labels.append(label)
            self.batched_filenames.append([filename])

 #ASVspoof
    def apply_adv_attack(self, batch_x, batch_y, target_spk, test_utterance):
        batch_x = batch_x.to(self.device)
        batch_y = batch_y.to(self.device)

        target_module = AttackEnum[self.adv_method1].target_module

        # ✅ 먼저 result 받아오기
        if target_module == "ASV":
            result = self.adv_class1.forward(target_spk=target_spk, test_utterance=test_utterance)
        elif target_module == "CM":
            result = self.adv_class1.forward(batch_x, batch_y)
        elif target_module == "BOTH":
            result = self.adv_class1.forward(target_spk=target_spk, test_utterance=test_utterance)
        else:
            raise ValueError(f"Unknown target module: {target_module}")

        # 실패했을 경우 None 리턴
        if result is None:
            print(f"[ERROR] Failed to generate adversarial example for {test_utterance}")
            return None

        # ✅ result unpack
        if target_module == "ASV":
            asv_before_score, asv_after_score, adv_audio = result
            adv_images = torch.tensor(adv_audio).unsqueeze(0).to(self.device)
            return None, None, asv_before_score, asv_after_score, adv_images

        elif target_module == "CM":
            cm_before_score, cm_after_score, adv_audio = result
            adv_images = torch.tensor(adv_audio).unsqueeze(0).to(self.device)
            return cm_before_score, cm_after_score, None, None, adv_images

        elif target_module == "BOTH":
            cm_before, cm_after, asv_before, asv_after, adv_audio = result
            adv_images = torch.tensor(adv_audio).unsqueeze(0).to(self.device)
            return cm_before, cm_after, asv_before, asv_after, adv_images

# VoxCeleb
    # def apply_adv_attack(self, batch_x, batch_y, enroll_file, test_file):
    #     batch_x = batch_x.to(self.device)
    #     batch_y = batch_y.to(self.device)

    #     target_module = AttackEnum[self.adv_method1].target_module

    #     # ✅ result 받아오기
    #     if target_module == "ASV":
    #         result = self.adv_class1.forward(enroll_file, test_file)
    #     elif target_module == "CM":
    #         result = self.adv_class1.forward(batch_x, batch_y)
    #     elif target_module == "BOTH":
    #         result = self.adv_class1.forward(enroll_file, test_file)
    #     else:
    #         raise ValueError(f"Unknown target module: {target_module}")

    #     if result is None:
    #         print(f"[ERROR] Failed to generate adversarial example for {test_file}")
    #         return None

    #     # ✅ result unpack
    #     if target_module == "ASV":
    #         asv_before_score, asv_after_score, adv_audio = result
    #         adv_images = torch.tensor(adv_audio).unsqueeze(0).to(self.device)
    #         return None, None, asv_before_score, asv_after_score, adv_images

    #     elif target_module == "CM":
    #         cm_before_score, cm_after_score, adv_audio = result
    #         adv_images = torch.tensor(adv_audio).unsqueeze(0).to(self.device)
    #         return cm_before_score, cm_after_score, None, None, adv_images

    #     elif target_module == "BOTH":
    #         cm_before, cm_after, asv_before, asv_after, adv_audio = result
    #         adv_images = torch.tensor(adv_audio).unsqueeze(0).to(self.device)
    #         return cm_before, cm_after, asv_before, asv_after, adv_images


    
    def transform(self):
        # get classifier_art
        # [TODO] load model here
        raise NotImplementedError
    
    def transform_load(self, input_dir, batch_size):
        # get classifier_art
        # [TODO] load model here
        raise NotImplementedError

        #ASVspoof
    
    def transform_batch(self, protocol_df):
        print(f"[DEBUG] protocol length = {len(protocol_df)}")
        attack_enum = AttackEnum[self.adv_method1]
        target_module = attack_enum.target_module  # "CM", "ASV", or "BOTH"

        adv_subdir = os.path.join(self.output_path, f"adv_examples/ab/{self.adv_method1.lower()}")
        os.makedirs(adv_subdir, exist_ok=True)

        # 로그 파일 경로
        log_file_path = os.path.join(
            adv_subdir, 
            f"score_{self.adv_method1.lower()}_{target_module.lower()}.txt"
        )

        # 로그 파일 헤더 초기화
        with open(log_file_path, "w") as log_file:
            if target_module == "ASV":
                log_file.write("utt1\tutt2\tasv_before\tasv_after\n")
            elif target_module == "CM":
                log_file.write("utt1\tutt2\tcm_before\tcm_after\n")
            elif target_module == "BOTH":
                log_file.write("utt1\tutt2\tcm_before\tcm_after\tasv_before\tasv_after\n")

        MAX_LEN = 32000

        for j in tqdm(range(len(protocol_df)), desc="Generating adversarial noise"):
            row = protocol_df.iloc[j]
            utt1, utt2, utt_type = row["utt1"], row["utt2"], row["type"]
            target_spk, test_utt = utt1, utt2

            # Skip non-targeted trials
            if target_module == "CM" and utt_type != "spoof":
                continue
            if target_module == "BOTH" and utt_type not in ["spoof", "nontarget"]:
                continue
            if target_module == "ASV" and utt_type != "nontarget":
                continue

            # Load test audio
            input_file = os.path.join(self.input_path, utt2 + ".flac")
            if not os.path.exists(input_file):
                print(f"[WARNING] File not found: {input_file}")
                continue

            audio_data, _ = librosa.load(input_file, sr=None)
            if len(audio_data) > MAX_LEN:
                audio_data = audio_data[:MAX_LEN]
            batch = torch.tensor(audio_data).unsqueeze(0).to(self.device)

            label = 0 if utt_type in ['spoof', 'nontarget'] else 1
            label_tensor = torch.tensor([label], dtype=torch.long).to(self.device)

            # Apply attack
            before_scores, after_scores, asv_before_score, after_cosine_sim, adv_images = \
                self.apply_adv_attack(batch, label_tensor, target_spk, test_utt)

            # Save adversarial audio
            output_audio_path = os.path.join(adv_subdir, f"{utt2}_{utt1}.wav")
            os.makedirs(os.path.dirname(output_audio_path), exist_ok=True)
            attacked_sample = adv_images.detach().cpu().numpy().flatten()
            sf.write(output_audio_path, attacked_sample, 16000, format='WAV')

            # 로그 저장
            with open(log_file_path, "a") as log_file:
                if target_module == "ASV":
                    log_file.write(f"{utt1}\t{utt2}\t{float(asv_before_score):.6f}\t{float(after_cosine_sim):.6f}\n")

                elif target_module == "CM":
                    for before_score, after_score in zip(
                        np.atleast_1d(before_scores),
                        np.atleast_1d(after_scores)
                    ):
                        log_file.write(f"{utt1}\t{utt2}\t{float(before_score):.6f}\t{float(after_score):.6f}\n")

                elif target_module == "BOTH":
                    for before_score, after_score in zip(
                        np.atleast_1d(before_scores),
                        np.atleast_1d(after_scores)
                    ):
                        log_file.write(f"{utt1}\t{utt2}\t{float(before_score):.6f}\t{float(after_score):.6f}\t"
                                    f"{float(asv_before_score):.6f}\t{float(after_cosine_sim):.6f}\n")
                        

    # #VoxCeleb

    # def transform_batch(self, protocol_df):
    #     attack_enum = AttackEnum[self.adv_method1]
    #     target_module = attack_enum.target_module  # "CM", "ASV", or "BOTH"

    #     adv_subdir = os.path.join(self.output_path, f"adv_examples/{self.adv_method1.lower()}")
    #     os.makedirs(adv_subdir, exist_ok=True)

    #     # 로그 파일 경로
    #     log_file_path = os.path.join(
    #         adv_subdir, 
    #         f"score_{self.adv_method1.lower()}_{target_module.lower()}.txt"
    #     )

    #     # 로그 파일 헤더 초기화
    #     with open(log_file_path, "w") as log_file:
    #         if target_module == "ASV":
    #             log_file.write("enroll\ttest\tasv_before\tasv_after\n")
    #         elif target_module == "CM":
    #             log_file.write("enroll\ttest\tcm_before\tcm_after\n")
    #         elif target_module == "BOTH":
    #             log_file.write("enroll\ttest\tcm_before\tcm_after\tasv_before\tasv_after\n")

    #     MAX_LEN = 32000

    #     for j in tqdm(range(len(protocol_df)), desc="Generating adversarial noise"):
    #         row = protocol_df.iloc[j]
    #         label, enroll, test = row["label"], row["enroll"], row["test"]

    #         enroll_path = os.path.join(self.input_path, enroll)
    #         test_path   = os.path.join(self.input_path, test)

    #         if not os.path.exists(test_path) or not os.path.exists(enroll_path):
    #             print(f"[WARNING] Missing file: {enroll_path} or {test_path}")
    #             continue

    #         # === 파일명 구성 (생성 전 체크) ===
    #         test_spk = os.path.splitext(test)[0] 
    #         enroll_name = os.path.splitext(enroll)[0]           
    #         out_path = os.path.join(
    #             adv_subdir, 
    #             f"{test_spk}_{enroll_name}.wav"
    #         )

    #         if os.path.exists(out_path):
    #             print(f"[SKIP] Already exists: {out_path}")
    #             continue

    #         # === 오디오 로드 ===
    #         audio_data, _ = librosa.load(test_path, sr=16000)
    #         if len(audio_data) > MAX_LEN:
    #             audio_data = audio_data[:MAX_LEN]
    #         batch = torch.tensor(audio_data).unsqueeze(0).to(self.device)

    #         label_tensor = torch.tensor([label], dtype=torch.long).to(self.device)

    #         # === 공격 적용 ===
    #         before_cm, after_cm, before_asv, after_asv, adv_audio = \
    #             self.apply_adv_attack(batch, label_tensor, enroll_path, test_path)

    #         if adv_audio is None:
    #             continue  # 실패시 스킵

    #         os.makedirs(os.path.dirname(out_path), exist_ok=True)
    #         sf.write(out_path, adv_audio.squeeze(0).cpu().numpy(), 16000)


            # # 로그 저장
            # with open(log_file_path, "a") as log_file:
            #     if target_module == "ASV":
            #         log_file.write(f"{enroll}\t{test}\t{float(before_asv):.6f}\t{float(after_asv):.6f}\n")

            #     elif target_module == "CM":
            #         log_file.write(f"{enroll}\t{test}\t{float(before_cm):.6f}\t{float(after_cm):.6f}\n")

            #     elif target_module == "BOTH":
            #         log_file.write(
            #             f"{enroll}\t{test}\t"
            #             f"{float(before_cm):.6f}\t{float(after_cm):.6f}\t"
            #             f"{float(before_asv):.6f}\t{float(after_asv):.6f}\n"
            #         )
